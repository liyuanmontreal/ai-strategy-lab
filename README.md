# ğŸ§  AI Strategy Lab | Mini Multi-Agent Battle Simulator

A lightweight **multi-agent tactical combat environment** designed for research in:

- Multi-Agent Reinforcement Learning (MARL)
- Tactical decision-making & emergent strategy
- Micro-control in RTS-style combat
- Agent cooperation & adversarial behavior

Inspired by **StarCraft micro battles**, tactical board games, and lightweight RTS AI systems â€” simple enough to train quickly, yet expressive enough to study strategy.

---

## âœ¨ Key Features

| Capability | Description |
|---|---|
Multi-agent adversarial environment | Red vs Blue combat squads  
Action model | Movement, attack, damage, death, victory  
Training support | Rule-based agents & PPO (SB3)  
Rendering | ASCII + GIF replay system  
Frameworks | PettingZoo + SuperSuit + Stable-Baselines3  
Extensibility | Ranged units, fog-of-war, terrain, LLM commander  

---

## ğŸ® Environment Overview

### ğŸ‘¥ Agents
Two teams:

- `red_0 ... red_N`
- `blue_0 ... blue_N`

### ğŸ¯ Action Space (9 discrete actions)

| ID | Action |
|---|---|
0 | Stay  
1 | Move up  
2 | Move down  
3 | Move left  
4 | Move right  
5 | Attack up  
6 | Attack down  
7 | Attack left  
8 | Attack right  

### â¤ï¸ Stats

| Attribute | Default |
|---|---|
HP | 3  
Grid | 15Ã—15 (configurable)  
Rewards | +1 kill, shaped micro-rewards  

### ğŸ”„ Observation

RGB grid representation:

- Friend channel  
- Enemy channel  
- Self-position channel  

---

## ğŸ“‚ Project Structure
envs/
â””â”€â”€ micro_v1.py # Core battle environment
utils/
â””â”€â”€ replay_recorder.py # GIF replay recorder
train/
â””â”€â”€ train_sb3_ppo.py # PPO training script
baselines/
â””â”€â”€ rule_based.py # Rule-based benchmark

Environment entry point:

```python
from envs.micro_v1 import env
e = env(grid_size=15, n_per_team=5)

ğŸš€ Getting Started
Install
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

Run rule-based agents
python baselines/rule_based.py

Train PPO agent
python train/train_sb3_ppo.py --grid-size 12 --units 4 --num-envs 2 --total-steps 200000

ğŸ¥ Generate Battle Replay GIF
python utils/replay_recorder.py --steps 400 --outfile fight.gif


Example:

Units advance, engage, take damage, die, and one side wins.

ğŸ§  Research Directions

Micro tactics: focus-fire, flanking, kiting

Emergent teamwork & coordination

LLM-guided reinforcement learning (â€œAI Commanderâ€)

Strategy curriculum & self-play evolution

Partial observability (fog-of-war)

ğŸ§© Roadmap
Version	Feature
âœ… v1	Movement, melee combat, replay, PPO
ğŸŸ¡ v2	Fog-of-war, vision, recurrent PPO (LSTM)
ğŸŸ¡ v3	Unit types: ranged / melee / healer
â¬œ v4	Terrain, cover, obstacles
â¬œ v5	Resource & build system
â¬œ v6	LLM tactical commander (high-level planning)
ğŸ› ï¸ Tech Stack
Category	Tools
RL	Stable-Baselines3 (PPO)
Multi-Agent Env	PettingZoo ParallelEnv
Vectorization	SuperSuit
Visualization	ASCII â†’ GIF (pygame WIP)
Logging (optional)	TensorBoard / Weights & Biases
ğŸ¤ Acknowledgements

PettingZoo

Stable-Baselines3

StarCraft AI research community

Multi-Agent RL literature

ğŸ“¬ Contact

Interested in:

RL / MARL

RTS AI experiments

Game-AI research

Lightweight custom environments

Letâ€™s connect!

